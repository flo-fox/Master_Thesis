---
author: "Florian Fox"
date: "`r Sys.Date()`"
title: 'Master Thesis: A Causal Test of the Law of 1/n and its Mechanisms -- Analysis'
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
---

# Load data and packages

```{r Packages}
library(tidyverse)
library(fixest)
library(rdrobust)
library(rdmulti)
library(rddensity)
library(rdd) # For the McCrary (2008) test
```

```{r Data}
data <-
  readRDS("data/gemeinderatswahlen_alldata.rds")
```

```{r Options}
options(scipen = 20)
theme_set(theme_minimal())
```

to do Liste

- clustered SE

Generating different data sets:

```{r}
d_bivariate <- data %>%
  select(ln_gross_expenditure_pc, total_seats, ags, year) %>% drop_na()
d_bivariate_iv <- data %>%
  select(ln_gross_expenditure_pc, total_seats, ags, year, total_seats_24_y_ago) %>% drop_na()
d_rdd_bivariate <- data %>%
  select(ln_gross_expenditure_pc, total_seats, inhabs_rel_to_cutoff, above_cutoff, inhabitants_treshold, year, ags, election_year, state) %>%
  drop_na(ln_gross_expenditure_pc, total_seats, inhabs_rel_to_cutoff)

d_controls <- data %>%
  select(ln_gross_expenditure_pc, total_seats, ags, year, inh_tot, unempl_rate, total_area_ha, share_working_age, kreisfreie_stadt, stadt, years_since_last_elec) %>%
  drop_na()
d_controls_iv <- data %>%
  select(ln_gross_expenditure_pc, total_seats, ags, year, total_seats_24_y_ago, inh_tot, unempl_rate, total_area_ha, share_working_age, kreisfreie_stadt, stadt, years_since_last_elec) %>%
  drop_na()
d_rdd <- data %>%
  select(ln_gross_expenditure_pc, total_seats, inhabs_rel_to_cutoff, above_cutoff, inhabitants_treshold, year, ags,
         inh_tot, unempl_rate, total_area_ha, share_working_age, kreisfreie_stadt, stadt, years_since_last_elec) %>%
  drop_na()
d_ttest <- data %>%
  select(ln_gross_expenditure_pc, inhabs_rel_to_cutoff) %>% drop_na()
```



# OLS regression

**Biases in OLS regression**

"However, this regression potentially suffers from reverse causality as well as omitted variable bias. Reverse causality may arise from the fact that a municipality with a larger public budget faces more complex budget negotiations and hence may need more councilors ([Pettersson-Lidbom, 2012](https://doi.org/10.1016/j.jpubeco.2011.07.004), p. 269). If this is indeed the case, one would expect β to overestimate the true causal effect of council size. Omitted variable bias may also be at play because even with a large number of control variables one can not completely rule out confounders ([Höhmann, 2017](https://doi.org/10.1007/s11127-017-0484-2), p. 347). Depending on the sign of the association with council size and public spending, this bias could go either way. More precisely, if voters who prefer a larger number of councilors also want higher spending, the relationship between council size and government size might be spuriously correlated ([Egger & Koethenbuerger, 2010](https://www.doi.org/10.1257/app.2.4.200), pp. 201, 204), again resulting in an overestimate of the true effect."

(Citation from my project study)

```{r}
fixest::feols(ln_gross_expenditure_pc ~ total_seats,
              vcov = "hetero", # heteroskedasticity-robust SEs
              data = d_bivariate)
# Adding variables stepwise using the `csw` function
fixest::feols(
  ln_gross_expenditure_pc ~ csw0(total_seats, inh_tot, unempl_rate, total_area_ha,
    share_working_age, kreisfreie_stadt, stadt, years_since_last_elec),
  vcov = "hetero",
  data = data
)
ols <- fixest::feols(
  ln_gross_expenditure_pc ~ total_seats + inh_tot + unempl_rate + total_area_ha +
    share_working_age + kreisfreie_stadt + stadt + years_since_last_elec,
  vcov = "hetero",
  data = d_controls
)
summary(ols, n = 20)
rm(ols)
```



# Panel regression

Ignoring the potential issue of bad controls, I am running some panel regressions with year and municipality as fixed effects.

`stadt` and `kreisfreie_stadt` have been dropped because of collinearity concerns.

```{r}
panel_bivariate <-
  fixest::feols(ln_gross_expenditure_pc ~ total_seats |
                  year + ags,
                data = d_bivariate)
summary(panel_bivariate, vcov = "hetero")
summary(panel_bivariate, vcov = "twoway")
# Adding variables stepwise using the `csw` function
fixest::feols(
  ln_gross_expenditure_pc ~ csw0(total_seats, inh_tot, unempl_rate, total_area_ha,
    share_working_age, years_since_last_elec) |
    year + ags,
  data = data,
  vcov = "twoway"
)
panel_multivariate <- fixest::feols(
  ln_gross_expenditure_pc ~ total_seats + inh_tot + unempl_rate + total_area_ha +
    share_working_age + years_since_last_elec |
    year + ags,
  data = d_controls
)
summary(panel_multivariate, vcov = "hetero", n = 20)
summary(panel_multivariate, vcov = "twoway", n = 20)
```

Using heteroskedasticity-robust SE, the coefficient on `total_seats` shrinks by 70-90 %, compared to the OLS model but remains statistically significantly positive. However, once I allow for clustered standard errors, the coefficient is only significant at the 10 % level.

```{r}
rm(list=ls(pattern="^panel_"))
```



# Instrumental variables regression

Try an an IV regression based on [R. Baqir (2002)](https://doi.org/10.1086/342804), using council size 24 years ago (Baqir: 30 years ago) as an instrument.

The analysis in this section is exclusively based on Bavaria data.

For the IV to be a valid research design, three assumptions need to hold:

- Relevance: The instrument $Z_i$ needs to be correlated with the endogenous regressor $W_i$, $cov(Z_i, W_i) \neq 0$. The correlation (Pearson) coefficient is $r=$
`r round(cor(data$total_seats, data$total_seats_24_y_ago, use = "pairwise.complete.obs"),2)`
so formally, this holds. However, I would question whether the correlation indeed is *too strong* as the first-stage fitted values might pick up some of the endogenous character of the endogenous regressor, yielding biased results.
- Validity -- Exclusion:
- Validity -- Exogeneity:

`years_since_last_elec` has been removed from the equation over collinearity concerns.

```{r}
iv_bivariate <-
  fixest::feols(ln_gross_expenditure_pc ~
                  1 | # 1 is the intercept
                  year + ags | # FE
                  total_seats ~ total_seats_24_y_ago,
                data = d_bivariate_iv)
summary(iv_bivariate, vcov = "hetero")
summary(iv_bivariate, vcov = "twoway")
```
```{r, eval = FALSE}
# Adding variables stepwise using the `csw` function
# Not working as desired
fixest::feols(
  ln_gross_expenditure_pc ~
    csw0(inh_tot + unempl_rate + total_area_ha + share_working_age) | # controls
    year + ags | # FE
    total_seats ~ total_seats_24_y_ago,
  # IV formula for 1st stage
  data = data,
  vcov = "twoway"
)
```
```{r}
iv_multivariate <- fixest::feols(
  ln_gross_expenditure_pc ~
    inh_tot + unempl_rate + total_area_ha + share_working_age | # controls
    year + ags | # FE
    total_seats ~ total_seats_24_y_ago,
  # IV formula for 1st stage
  data = d_controls_iv
)
summary(iv_multivariate, vcov = "hetero")
summary(iv_multivariate, vcov = "twoway")
```

["Wu-Hausman tests that IV is just as consistent as OLS, and since OLS is more efficient, it would be preferable. The null here is that they are equally consistent"](https://stats.stackexchange.com/questions/134789/interpretation-of-ivreg-diagnostics-in-r). The null is rejected at the 10 % level here, which is "good" from the IV perspective.

The IV coefficients on `total_seats` are positive but far from any commonly accepted level of statistical significance. The effect size appears way to large...


```{r}
rm(list=ls(pattern="^iv_"))
```



# Regression discontinuity design -- Normalized Cutoff

Sharp RDD mit "intention-to-treat" effect?

Treating all of the cutoffs as one requires a harmonization. This happens according to Egger and Koethenbuerger (2010):

Using the Egger and Koethenbuerger (2010) calculation steps:
$$\tilde{N_i} = N_i/N_d$$
with $N_i$ as the relevant actual population size and $N_d$ as the respective thresholds.


## t test

The most simple thing to do is a simple t test for differences.

```{r}
# 0.1 around cutoff
d_ttest2 <- d_ttest %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.1) # Choose bandwidth
nrow(d_ttest2) / nrow(data) # Share of observations close to threshold
t.test(log(d_ttest2$ln_gross_expenditure_pc[d_ttest2$inhabs_rel_to_cutoff < 0]),
       log(d_ttest2$ln_gross_expenditure_pc[d_ttest2$inhabs_rel_to_cutoff >= 0]))
# 0.05 around cutoff
d_ttest2 <- d_ttest %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.05) # Choose bandwidth
nrow(d_ttest2) / nrow(data) # Share of observations close to threshold
t.test(log(d_ttest2$ln_gross_expenditure_pc[d_ttest2$inhabs_rel_to_cutoff < 0]),
       log(d_ttest2$ln_gross_expenditure_pc[d_ttest2$inhabs_rel_to_cutoff >= 0]))
# 0.01 around cutoff
d_ttest2 <- d_ttest %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.025) # Choose bandwidth
nrow(d_ttest2) / nrow(data) # Share of observations close to threshold
t.test(log(d_ttest2$ln_gross_expenditure_pc[d_ttest2$inhabs_rel_to_cutoff < 0]),
       log(d_ttest2$ln_gross_expenditure_pc[d_ttest2$inhabs_rel_to_cutoff >= 0]))
```

The results are not really consistent when it comes to statistical significance. The difference between both sides also heavily depends on the bandwidth chosen.
Keep in mind, however, that a t test is not exactly appropriate due to the fuzzy nature of the setting.

```{r}
rm(d_ttest2)
```


## Manual IV regression

This is how it is done according to ["The Effect" of Nick Huntington-Klein, chapter 20](https://theeffectbook.net/ch-RegressionDiscontinuity.html):
```{r}
df <- d_rdd_bivariate %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.05) # Choose bandwidth
nrow(df) / nrow(data) # Share of observations close to threshold
rdd <- fixest::feols(ln_gross_expenditure_pc ~
                     1  | # controls
                     inhabitants_treshold + year + ags | # fixed-effect controls
                     # First stage regression:
                     inhabs_rel_to_cutoff*total_seats ~ # Instrument our standard RDD...
                     inhabs_rel_to_cutoff*above_cutoff, # ... with being above the cutoff
                     data = df)
summary(rdd, vcov = "hetero")
summary(rdd, vcov = "threeway")
# First stage:
summary(rdd, stage = 1, vcov = "threeway")
# More tests:
fixest::fitstat(rdd, ~ ivf1 + ivwald1 + ivf2 + ivwald2)
rm(df, rdd)
```

This is how I would do it:
```{r}
df <- d_rdd_bivariate %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.05) # Choose bandwidth
nrow(df) / nrow(data) # Share of observations close to threshold
rdd <- fixest::feols(ln_gross_expenditure_pc ~
                     1  | # controls
                     inhabitants_treshold + year + ags | # fixed-effect controls
                     # First stage regression:
                     total_seats ~ # Instrument our standard RDD...
                     above_cutoff, # ... with being above the cutoff
                     data = df)
summary(rdd, vcov = "hetero")
summary(rdd, vcov = "threeway")
# First stage:
summary(rdd, stage = 1, vcov = "threeway")
# More tests:
fixest::fitstat(rdd, ~ ivf1 + ivwald1 + ivf2 + ivwald2)
rm(df, rdd)
```

```{r}
df <- d_rdd_bivariate %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.05) # Choose bandwidth
nrow(df) / nrow(data) # Share of observations close to threshold
rdd <- fixest::feols(ln_gross_expenditure_pc ~
                     inhabs_rel_to_cutoff  | # controls
                     inhabitants_treshold + year + ags | # fixed-effect controls
                     # First stage regression:
                     total_seats ~ # Instrument our standard RDD...
                     above_cutoff, # ... with being above the cutoff
                     data = df)
summary(rdd, vcov = "hetero")
summary(rdd, vcov = "threeway")
# First stage:
summary(rdd, stage = 1, vcov = "threeway")
rm(df, rdd)
```

Include polynomials:

```{r}
df <- d_rdd_bivariate %>%
  filter(abs(inhabs_rel_to_cutoff) < 0.05) # Choose bandwidth
nrow(df) / nrow(data) # Share of observations close to threshold
rdd <- fixest::feols(ln_gross_expenditure_pc ~
                     1  | # controls
                     inhabitants_treshold + year + ags | # fixed-effect controls
                     # First stage regression:
                     poly(inhabs_rel_to_cutoff, 6)*total_seats ~ # Instrument our standard RDD...
                     poly(inhabs_rel_to_cutoff, 6)*above_cutoff, # ... with being above the cutoff
                     data = df)
summary(rdd, vcov = "threeway")
rm(df, rdd)
```


Vary polynomials, bandwidths etc.

Add controls



## `rdrobust` package

```{r}
#df <- d_rdd_bivariate %>%
#  filter(abs(inhabs_rel_to_cutoff) < 0.05) # Choose bandwidth
#rdd <- rdrobust::rdrobust(y = log(data2$gross_expenditure), x = data2$inhabs_rel_to_cutoff, fuzzy = data2$jump_in_seats_cutoff)
#rdd <- rdrobust::rdrobust(y = log(data2$gross_expenditure), x = data2$inhabs_rel_to_cutoff, fuzzy = ifelse(data2$inhabs_rel_to_cutoff>=0,data2$jump_in_seats_cutoff, 0))
#rdd <- rdrobust::rdrobust(y = log(data2$gross_expenditure), x = data2$inhabs_rel_to_cutoff)
rdd <- rdrobust::rdrobust(y = d_rdd_bivariate$ln_gross_expenditure_pc, x = d_rdd_bivariate$inhabs_rel_to_cutoff, fuzzy = d_rdd_bivariate$total_seats, all = TRUE)
summary(rdd)
#rdrobust::rdplot(y = data2$gross_expenditure, x = data2$inhabs_rel_to_cutoff, fuzzy = data2$total_seats)
```

Second stage interpretation if loglin: coefficient times 100 %.

**Explanation of the output required! (which coefficients, SE, CI to read)**

Second stage interpretation if loglog: 1 % increase in council size = $coefficient$ % increase in spending.

```{r}
rdrobust::rdplot(y = d_rdd_bivariate$ln_gross_expenditure_pc, x = d_rdd_bivariate$inhabs_rel_to_cutoff)
ggplot(data = d_rdd_bivariate) +
  geom_density(aes(inhabs_rel_to_cutoff))
ggplot(data = d_rdd_bivariate) +
  geom_point(aes(x = inhabs_rel_to_cutoff, y = ln_gross_expenditure_pc))
```


Investigating mass points: to do

```{r}
sum(!is.na(data$inhabs_rel_to_cutoff)) # Number of valid observations of running variable
length(unique(data$inhabs_rel_to_cutoff)) # Number of unique values of running variable
sum(!is.na(data$inhabs_rel_to_cutoff)) / length(unique(data$inhabs_rel_to_cutoff)) # 18 observations per value
```



## Mechanisms


### Different fiscal policy variables

etc.


## Checking assumptions

### McCrary (2008) Sorting test

I am only using election years in order to avoid multiple observations on the running variable (also for the `rddensity` check).

```{r}
dens_test_d_rdd_bivariate <- d_rdd_bivariate %>%
  filter(election_year == 1 | year == 2002)
```

McCrary test for all states pooled (with running variable values occuring multiple times):
```{r}
rdd::DCdensity(runvar = data$inhabs_rel_to_cutoff, cutpoint = 0, plot = TRUE, verbose = TRUE)
```

```{r}
rdd::DCdensity(runvar = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff, cutpoint = 0, plot = TRUE, verbose = TRUE)
```


Export plot:
```{r}
pdf(file = "plots/mccrary.pdf", width = 10, height = 10*9/16)
rdd::DCdensity(runvar = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff, cutpoint = 0, plot = TRUE, verbose = TRUE)
title(xlab = "Running variable (Population relative to thresholds)")
abline(v=0)
dev.off()
```

The test for the overall data is significant (i. e. there is a jump at the threshold). But this may be due to almost all values of the running variable occuring multiple times (given the nature of the data set). Hence, the "real" test on the data set is not statistically significant any more.


### `rddensity` discontinuity test

```{r}
rddensity_disc_test <- rddensity::rddensity(X = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff, c = 0)
summary(rddensity_disc_test)
p <- rddensity::rdplotdensity(rddensity_disc_test, X = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff, xlabel = "Running variable (Population relative to threshold)")
```

Export plot
```{r}
pdf(file = "plots/rddensity.pdf", width = 10, height = 10*9/16)
p
dev.off()
rm(p)
```

### Testing for sorting in different entities, time periods, thresholds...

McCrary and `rddensity` tests for each state separately.

- McCrary: HE & TH have (positive) significant jumps at the cutoff.
- `rddensity`: BW & SA show signs of significant sorting at the cutoff, positively and negatively, respectively.

```{r}
all_states <- dens_test_d_rdd_bivariate %>%
  distinct(state) %>%
  filter(state != 3 & state != 12 & state != 13 & state != 15) %>% # Remove state with no/too few obs
  pull()
for (i in all_states){
  df_subset <- dens_test_d_rdd_bivariate %>% filter(state == i) %>% drop_na(inhabs_rel_to_cutoff)
  out1 <- rdd::DCdensity(runvar = df_subset$inhabs_rel_to_cutoff, cutpoint = 0, plot = FALSE, ext.out = TRUE)
  out2 <- rddensity::rddensity(X = df_subset$inhabs_rel_to_cutoff)
  print(paste0("McCrary -- State: ", i, ", jump: ", out1$theta, ", p value: ", out1$p, ", n: ", length(df_subset$inhabs_rel_to_cutoff)))
  print(paste0("rddensity -- State: ", i, ", jump: ", out2$hat$diff, ", p value: ", out2$test$p_jk, ", n: ", length(df_subset$inhabs_rel_to_cutoff)))
}
rm(all_states, df_subset, out1, out2)
```

McCrary and `rddensity` tests for each year separately:

- McCrary: No significant discontinuities here.
- `rddensity`: 2003 and 2004 (the latter was an important election year on the local level) show signs of sorting to the right at the 10% level.


```{r}
all_years <- dens_test_d_rdd_bivariate %>%
  distinct(year) %>%
  arrange(year) %>%
  pull()
for (i in all_years){
  df_subset <- dens_test_d_rdd_bivariate %>% filter(year == i) %>% drop_na(inhabs_rel_to_cutoff)
  out1 <- rdd::DCdensity(runvar = df_subset$inhabs_rel_to_cutoff, cutpoint = 0, plot = FALSE, ext.out = TRUE)
  out2 <- rddensity::rddensity(X = df_subset$inhabs_rel_to_cutoff)
  print(paste0("McCrary -- Year: ", i, ", jump: ", out1$theta, ", p value: ", out1$p, ", n: ", length(df_subset$inhabs_rel_to_cutoff)))
  print(paste0("rddensity -- Year: ", i, ", jump: ", out2$hat$diff, ", p value: ", out2$test$p_jk, ", n: ", length(df_subset$inhabs_rel_to_cutoff)))
}
rm(all_years, df_subset, out1, out2)
```

Testing at every threshold separately:

- McCrary test: 300, 2,000, and 10,000 significant at least at the 10 % significance level.
- `rddensity` reveals sorting to the left at the 15,000 threshold.

```{r}
all_th <- dens_test_d_rdd_bivariate %>%
  filter(inhabitants_treshold <= 30000 | inhabitants_treshold == 50000 |
           inhabitants_treshold == 100000 | inhabitants_treshold == 150000) %>%
  # Test works only for those values
  distinct(inhabitants_treshold) %>%
  arrange(inhabitants_treshold) %>%
  pull()
for (i in all_th){
  df_subset <- dens_test_d_rdd_bivariate %>%
    filter(inhabitants_treshold == i) %>%
    drop_na(inhabs_rel_to_cutoff)
  state <- unique(df_subset$state) %>% paste(collapse = ",")
  out1 <- rdd::DCdensity(runvar = df_subset$inhabs_rel_to_cutoff, cutpoint = 0, plot = FALSE, ext.out = TRUE)
  out2 <- rddensity::rddensity(X = df_subset$inhabs_rel_to_cutoff)
  print(paste0("McCrary -- Threshold: ", i, ", state(s): ", state, ", jump: ", out1$theta, ", p value: ", out1$p, ", n: ", length(df_subset$inhabs_rel_to_cutoff)))
  print(paste0("rddensity -- Threshold: ", i, ", state(s): ", state, ", jump: ", out2$hat$diff, ", p value: ", out2$test$p_jk, ", n: ", length(df_subset$inhabs_rel_to_cutoff)))
}
rm(all_th, df_subset, out1)
```

Placebo McCrary (2008) test: Even the placebo thresholds are statistically significant in many cases. Maybe the discontinuities above are just a matter of statistical power?
```{r}
# "Structured" approach
cuts <- seq(from = -(2+2/3), to = 2/3, by = 1/6) # Values chosen such that code does run
length <- length(cuts)
cutoff <- list()
pvalue1 <- list()
pvalue2 <- list()
for (i in 1:length){
  cutoff[i] <- cuts[i]
  pvalue1[i] <- rdd::DCdensity(runvar = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff,
                              cutpoint = cuts[i], plot = FALSE)
  pvalue2[i] <- rddensity::rddensity(X = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff, c = cuts[i])$test$p_jk
}
sorting <- tibble(cutoff = cutoff, pvalue_mccrary = pvalue1, pvalue_rddensity = pvalue2) %>%
  unnest() %>%
  arrange(pvalue_mccrary)
sorting
rm(cutoff, cuts, i, length, pvalue1, pvalue2, sorting)
```
```{r}
# "Random" approach
set.seed(234) # Value chosen such that rdd:DCdensity actually runs without error -> bias?
random_z_score <- rnorm(50)/2 # Neither randomly nor z-distributed, but roughly around my cutoff
cutoff <- list()
pvalue_mccrary <- list()
jump_rddensity <- list()
pvalue_rddensity <- list()
for (i in 1:50){
  #print(paste("Cutoff at:", random_z_score[i]))
  cutoff[i] <- random_z_score[i]
  pvalue_mccrary[i] <- rdd::DCdensity(runvar = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff,
                                       cutpoint = random_z_score[i], plot = FALSE)
  out2 <- rddensity::rddensity(X = dens_test_d_rdd_bivariate$inhabs_rel_to_cutoff,
                               c = random_z_score[i])
  jump_rddensity[i] <- out2$hat$diff
  pvalue_rddensity[i] <- out2$test$p_jk
}
sorting <- tibble(cutoff, pvalue_mccrary, jump_rddensity, pvalue_rddensity) %>%
  unnest() %>%
  arrange(cutoff)
sorting
ggplot2::ggplot(sorting) + geom_histogram(aes(pvalue_mccrary))
ggplot2::ggplot(sorting) + geom_histogram(aes(pvalue_rddensity))
rm(cutoff, i, out2, pvalues, random_z_score)
```






Strangely, the number of observations with p values smaller than 10 % is higher than one would expect. I do not have an explanation for this.


```{r}
rm(dens_test_d_rdd_bivariate)
```

### Balance tests


# Robustness checks

Different specifications (e. g. varying bandwidth, polynomials), 

placebo etc

## Placebo

## Sensitivity analysis

How does estimate react to different bandwidths?


## Mass points

Lee and Card 2008 procedure: Clustering at the respective dicrete values of the running variable

--> RDhonest???


## Treatment Effect Heterogeneity

Hsu Shen nd


# Regression Discontinuity Design -- Exploiting the multiple-cutoff setting

Versuch 1

```{r, eval = FALSE}
set.seed(123)
data2 <- data %>%
  filter(state == 9, total_seats <= 24) %>%
  select(ln_gross_expenditure_pc, inhabitants_treshold, exact_pop, total_seats) %>%
  drop_na() %>%
  slice_sample(n = 8000)
rdd <- rdmulti::rdms(Y = data2$ln_gross_expenditure_pc, X = data2$exact_pop, fuzzy = data2$total_seats, C = data2$inhabitants_treshold)
#rdd <- rdmulti::rdms(Y = data2$ln_gross_expenditure_pc, X = data2$exact_pop, C = data$inhabitants_treshold)
#unique(data2$inhabitants_treshold), fuzzy = data2$total_seats)
```

Versuch 2:

```{r, eval = FALSE}
data2 <- data %>%
  filter(gross_expenditure > 0, state == 9) %>%
  select(gross_expenditure, exact_pop, inhabitants_treshold, total_seats) %>%
  drop_na() %>%
  slice(50:500)
rdrobust::rdrobust(y = data2$gross_expenditure, x = data2$exact_pop, c = 0, fuzzy = data2$total_seats, all = TRUE)
```

```{r, eval=FALSE}
rdms(Y,X,cvec)
```



```{r, eval = FALSE}
rdmulti::rdmcplot(Y = log(data2$gross_expenditure), X = data2$exact_pop, C = data2$inhabitants_treshold)
```



# Concluding Remarks

For references, see the paper.

```{r}

```
